# -*- coding: utf-8 -*-
"""inception.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1532X3PeFVfShlMcUPwuiNnYuLLHsE6Wk
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

!pip uninstall tensorflow
!pip install tensorflow==2.10.0

from tensorflow.keras.models import load_model

# Load the pre-trained model
model = load_model('/content/gdrive/My Drive/inception_fine_tuned.keras')

# Print the model summary
#model.summary()

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

# Define the paths to your dataset on Google Drive
data_dir = '/content/gdrive/My Drive/dataset'

# Data preprocessing and augmentation
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2  # Split the data into training and validation sets
)

# Create train and validation data generators
train_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(299, 299),  # InceptionV3's input size
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

valid_generator = datagen.flow_from_directory(
    data_dir,
    target_size=(299, 299),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

# Load the InceptionV3 model with pre-trained weights (include_top=False to exclude the fully connected layers)
base_model = InceptionV3(weights='imagenet', include_top=False)

# Add your custom layers on top of the pre-trained model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)  # Add additional dense layers if needed
predictions = Dense(10, activation='softmax')(x)  # 10 classes for LULC

# Create the final model for training
model = Model(inputs=base_model.input, outputs=predictions)

# Find the index of the layer just before the GlobalAveragePooling2D layer
index_to_unfreeze = model.layers.index(model.get_layer('global_average_pooling2d')) - 1

# Unfreeze only that layer
model.layers[index_to_unfreeze].trainable = True

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
checkpoint = ModelCheckpoint('/content/gdrive/My Drive/inception_fine_tuned.kera s', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)
early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

# Train the model for 10 epochs
history = model.fit(
    train_generator,
    validation_data=valid_generator,
    epochs=10,  # Train for 10 epochs
    callbacks=[checkpoint, early_stopping]
)

# Print loss and accuracy for each training epoch
for epoch, (loss, accuracy) in enumerate(zip(history.history['loss'], history.history['accuracy'])):
    print(f"Epoch {epoch + 1} / 10 - Loss: {loss:.4f}, Accuracy: {accuracy:.4f}")

# Print validation loss and accuracy at the end
validation_loss, validation_accuracy = model.evaluate(valid_generator)
print(f"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}")

# Save the final model
model.save('/content/gdrive/My Drive/inception_fine_tuned_final.keras')

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Load the Excel file into a DataFrame
excel_file_path = 'inception_features.xlsx'  # Replace with your file path
data = pd.read_excel(excel_file_path)

# Extract the 'category' column to keep it unchanged
category_column = data['category']

# Apply min-max normalization to the feature columns
numerical_data = data.drop(columns=['category'])
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(numerical_data)

# Convert the normalized data back to a DataFrame
normalized_data = pd.DataFrame(normalized_data, columns=numerical_data.columns)

# Add the 'category' column back to the normalized data
normalized_data['category'] = category_column

# Specify the path to save the normalized Excel file
normalized_excel_file_path = 'inception_features_normalized.xlsx'  # Replace with your desired path

# Save the normalized data to a new Excel file
normalized_data.to_excel(normalized_excel_file_path, index=False)

print(f"Normalized data saved to {normalized_excel_file_path}")

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model

# Load the pre-trained model
model = load_model('/content/gdrive/My Drive/inception_fine_tuned_final.keras')

# Directory containing your class subdirectories
data_dir = '/content/gdrive/My Drive/dataset'

# Create a list to store features
features_list = []
categories_list = []

# Define the output Excel file path
output_excel_file = 'inception_features.xlsx'

# Access the dense layer with 1024 features
dense_layer = model.get_layer('dense').output

# Create a feature extraction model
feature_extraction_model = Model(inputs=model.input, outputs=dense_layer)

# Loop through all the images in the dataset directory
for class_folder in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_folder)
    for image_file in os.listdir(class_path):
        # Load and preprocess the image
        img_path = os.path.join(class_path, image_file)
        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))
        img_array = tf.keras.preprocessing.image.img_to_array(img)
        img_array = tf.keras.applications.inception_v3.preprocess_input(img_array)
        img_array = np.expand_dims(img_array, axis=0)

        # Get features from the 'dense' layer
        features = feature_extraction_model.predict(img_array)
        features = features[0]  # Extract features from the flattened array

        # Append the features to the list
        features_list.append(features)
        categories_list.append(class_folder)  # Add the category label

# Convert the lists of features and categories to Pandas DataFrames
features_df = pd.DataFrame(features_list)
categories_df = pd.DataFrame({'category': categories_list})

# Concatenate the two DataFrames to include the 'category' column
result_df = pd.concat([features_df, categories_df], axis=1)

# Explicitly convert numerical columns to float data type
numerical_cols = [col for col in result_df.columns if str(col).startswith('feature')]
result_df[numerical_cols] = result_df[numerical_cols].astype(float)

# Save the DataFrame to an Excel file (XLSX format)
result_df.to_excel(output_excel_file, index=False)

print(f"Results saved to {output_excel_file}")

# CONVERT XLSX TO CSV

import pandas as pd

# Load the Excel file
excel_file_path = '/content/inception_features_normalized.xlsx'  # Replace with your input XLSX file path
data = pd.read_excel(excel_file_path)

# Specify the path to save the CSV file
csv_file_path = '/content/inception_features_normalized.csv'  # Replace with your desired output CSV file path

# Save the data to a CSV file
data.to_csv(csv_file_path, index=False)

print(f"Data saved to {csv_file_path} in CSV format.")

"""



below is the testing code



"""



"""normalization"""